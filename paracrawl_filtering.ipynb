{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 5796581,
     "sourceType": "datasetVersion",
     "datasetId": 3285114
    },
    {
     "sourceId": 5841404,
     "sourceType": "datasetVersion",
     "datasetId": 2967582
    }
   ],
   "dockerImageVersionId": 30476,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install contractions\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import langid\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ],
   "metadata": {
    "id": "U5I73DExuwyu",
    "outputId": "ab7e70b5-7c75-4663-e4b8-501b79258b50",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:42:23.988917Z",
     "iopub.execute_input": "2023-06-19T09:42:23.989535Z",
     "iopub.status.idle": "2023-06-19T09:42:46.083043Z",
     "shell.execute_reply.started": "2023-06-19T09:42:23.989501Z",
     "shell.execute_reply": "2023-06-19T09:42:46.081929Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "paracrawl = pd.read_csv(\"data/paracrawl.txt\", encoding='utf-8', on_bad_lines='skip', sep='\\t', names=['English', 'Gaeilge'], dtype=str)\n",
    "paracrawl.head(5)"
   ],
   "metadata": {
    "id": "pywLOYhXuwyw",
    "outputId": "ab47954e-6cdb-47cf-999a-82092933de85",
    "execution": {
     "iopub.status.busy": "2023-06-04T13:46:41.674233Z",
     "iopub.execute_input": "2023-06-04T13:46:41.675061Z",
     "iopub.status.idle": "2023-06-04T13:46:43.492910Z",
     "shell.execute_reply.started": "2023-06-04T13:46:41.675015Z",
     "shell.execute_reply": "2023-06-04T13:46:43.491490Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                  English  \\\n8234    Should the offence be repeated, the President ...   \n41449   The Bureau shall adopt the guidelines for the ...   \n41154                        PUBLIC RECORD OF PROCEEDINGS   \n81314   In order to ensure that the conditions for rec...   \n53640   The entry summary declaration shall be lodged ...   \n...                                                   ...   \n52514                            Provision of information   \n148007                         Help me move this weekend.   \n74336   RULES CONCERNING MARKETING AND PRODUCER ORGANI...   \n15705                        the President of Parliament,   \n62311   The Programme should take into account the dua...   \n\n                                                  Gaeilge  \n8234    Beidh díospóireacht ghinearálta ann i ndiaidh ...  \n41449   Glacfaidh an Biúró treoirlínte do na Caestóirí...  \n41154                     TAIFEAD POIBLÍ AR NA hIMEACHTAÍ  \n81314   Chun a áirithiú go gcomhlíonfar na coinníollac...  \n53640   Ní mór an dearbhú iontrála achomair a thaiscea...  \n...                                                   ...  \n52514                             Faisnéis a chur ar fáil  \n148007    Cuidigh liom bogadh an deireadh seachtaine seo.  \n74336   RIALACHA MAIDIR LE MARGAÍOCHT AGUS EAGRAÍOCHTA...  \n15705                          - Uachtarán na Parlaiminte  \n62311   Le clár tacaíochta d'earnálacha an chultúir ag...  \n\n[150982 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Gaeilge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8234</th>\n      <td>Should the offence be repeated, the President ...</td>\n      <td>Beidh díospóireacht ghinearálta ann i ndiaidh ...</td>\n    </tr>\n    <tr>\n      <th>41449</th>\n      <td>The Bureau shall adopt the guidelines for the ...</td>\n      <td>Glacfaidh an Biúró treoirlínte do na Caestóirí...</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>PUBLIC RECORD OF PROCEEDINGS</td>\n      <td>TAIFEAD POIBLÍ AR NA hIMEACHTAÍ</td>\n    </tr>\n    <tr>\n      <th>81314</th>\n      <td>In order to ensure that the conditions for rec...</td>\n      <td>Chun a áirithiú go gcomhlíonfar na coinníollac...</td>\n    </tr>\n    <tr>\n      <th>53640</th>\n      <td>The entry summary declaration shall be lodged ...</td>\n      <td>Ní mór an dearbhú iontrála achomair a thaiscea...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>52514</th>\n      <td>Provision of information</td>\n      <td>Faisnéis a chur ar fáil</td>\n    </tr>\n    <tr>\n      <th>148007</th>\n      <td>Help me move this weekend.</td>\n      <td>Cuidigh liom bogadh an deireadh seachtaine seo.</td>\n    </tr>\n    <tr>\n      <th>74336</th>\n      <td>RULES CONCERNING MARKETING AND PRODUCER ORGANI...</td>\n      <td>RIALACHA MAIDIR LE MARGAÍOCHT AGUS EAGRAÍOCHTA...</td>\n    </tr>\n    <tr>\n      <th>15705</th>\n      <td>the President of Parliament,</td>\n      <td>- Uachtarán na Parlaiminte</td>\n    </tr>\n    <tr>\n      <th>62311</th>\n      <td>The Programme should take into account the dua...</td>\n      <td>Le clár tacaíochta d'earnálacha an chultúir ag...</td>\n    </tr>\n  </tbody>\n</table>\n<p>150982 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/snguyenthanh/better_profanity/blob/master/better_profanity/profanity_wordlist.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "profanity = open(\"profanity.txt\").read().splitlines()\n",
    "\n",
    "def filtering(en, ga):\n",
    "    en, ga = str(en), str(ga)\n",
    "    i = len(en)\n",
    "    j = len(ga)\n",
    "    \n",
    "    # length matching as per the Speechmatics Parallel Corpus Filtering System for WMT18\n",
    "    if len(en) < 100:\n",
    "        if 6 * i > j and i < 6 * j:\n",
    "            if i < 3 or j < 3 or (i < 2.2 * j and j < 2.2 * i):\n",
    "                if i < 10 or j < 10 or (i < 2 * j and j < 2 * i):\n",
    "\n",
    "                    # removing profanity \n",
    "                    if not any([word in en for word in profanity]):\n",
    "\n",
    "                            # is it Irish?\n",
    "                            if langid.classify(ga)[0] == 'ga':\n",
    "\n",
    "                                return True\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "did you forget parentheses around the comprehension target? (3172227818.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    para = [en, ga for en, ga in zip(paracrawl[\"English\"], paracrawl[\"Gaeilge\"]) if filtering(en, ga)]\u001B[0m\n\u001B[1;37m            ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m did you forget parentheses around the comprehension target?\n"
     ]
    }
   ],
   "source": [
    "pairs = [[en, ga] for en, ga in zip(paracrawl[\"English\"], paracrawl[\"Gaeilge\"]) if filtering(en, ga)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T12:45:25.714172Z",
     "start_time": "2023-11-23T12:45:25.709393800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preproc_en(text):\n",
    "    x = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-záéíóú ])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(text))\n",
    "    return \" \".join(nltk.word_tokenize(contractions.fix(x.lower())))\n",
    "\n",
    "def preproc_ga(text):\n",
    "    x = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-záéíóú ])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(text))\n",
    "    return \"[start] \" + \" \".join(nltk.word_tokenize(x.lower())) + \" [end]\"\n",
    "\n",
    "pairs[\"English\"] = pairs[\"English\"].apply(preproc_en)\n",
    "pairs[\"Gaeilge\"] = pairs[\"Gaeilge\"].apply(preproc_ga)\n",
    "\n",
    "pairs.head(5)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:59.784510Z",
     "iopub.execute_input": "2023-06-19T09:46:59.784900Z",
     "iopub.status.idle": "2023-06-19T09:46:59.800966Z",
     "shell.execute_reply.started": "2023-06-19T09:46:59.784861Z",
     "shell.execute_reply": "2023-06-19T09:46:59.800096Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   English  \\\n2429244  if you are selected as a member of the rt audi...   \n1557228  with the announcement today of government depa...   \n1104445   you can buypurchase stth112 right here right now   \n2314715  tim has three sons and his wife karla of 21 years   \n1140669  room service goradias lords inn shirdi shirdi ...   \n...                                                    ...   \n2292948  corporate authors directorategeneral for clima...   \n2734072  construction world cwindia 7h7 hours ago more ...   \n2521924  this could be when you have a family crisis wh...   \n1327842  ag péinteáil an tí is a picture book in the co...   \n2079717          misakiverified account ichuofficial jul 1   \n\n                                                   Gaeilge  \n2429244  [start] má roghnaítear tú ar bhall de chomhair...  \n1557228  [start] bhí súil ag an gcomhdháil agus na rann...  \n1104445  [start] is féidir leat a cheannach a cheannach...  \n2314715  [start] tá trí mhac agus a bhean chéile karla ...  \n1140669  [start] goradias lords inn shirdi shirdi bia d...  \n...                                                    ...  \n2292948  [start] dar corparáideach an coimisiún eorpach...  \n2734072  [start] construction world cwindia 7 u7 nuaire...  \n2521924  [start] dfhéadfadh géarchéim a bheith ann i do...  \n1327842  [start] pictiúrleabhar is ea fiacail mháire sa...  \n2079717  [start] rinne cuntas deimhnithe ichuofficial [...  \n\n[3092956 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Gaeilge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2429244</th>\n      <td>if you are selected as a member of the rt audi...</td>\n      <td>[start] má roghnaítear tú ar bhall de chomhair...</td>\n    </tr>\n    <tr>\n      <th>1557228</th>\n      <td>with the announcement today of government depa...</td>\n      <td>[start] bhí súil ag an gcomhdháil agus na rann...</td>\n    </tr>\n    <tr>\n      <th>1104445</th>\n      <td>you can buypurchase stth112 right here right now</td>\n      <td>[start] is féidir leat a cheannach a cheannach...</td>\n    </tr>\n    <tr>\n      <th>2314715</th>\n      <td>tim has three sons and his wife karla of 21 years</td>\n      <td>[start] tá trí mhac agus a bhean chéile karla ...</td>\n    </tr>\n    <tr>\n      <th>1140669</th>\n      <td>room service goradias lords inn shirdi shirdi ...</td>\n      <td>[start] goradias lords inn shirdi shirdi bia d...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2292948</th>\n      <td>corporate authors directorategeneral for clima...</td>\n      <td>[start] dar corparáideach an coimisiún eorpach...</td>\n    </tr>\n    <tr>\n      <th>2734072</th>\n      <td>construction world cwindia 7h7 hours ago more ...</td>\n      <td>[start] construction world cwindia 7 u7 nuaire...</td>\n    </tr>\n    <tr>\n      <th>2521924</th>\n      <td>this could be when you have a family crisis wh...</td>\n      <td>[start] dfhéadfadh géarchéim a bheith ann i do...</td>\n    </tr>\n    <tr>\n      <th>1327842</th>\n      <td>ag péinteáil an tí is a picture book in the co...</td>\n      <td>[start] pictiúrleabhar is ea fiacail mháire sa...</td>\n    </tr>\n    <tr>\n      <th>2079717</th>\n      <td>misakiverified account ichuofficial jul 1</td>\n      <td>[start] rinne cuntas deimhnithe ichuofficial [...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3092956 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train = pairs.sample(frac=0.8)\n",
    "extra = pairs.drop(train.index)\n",
    "val = extra.sample(frac=0.75)\n",
    "ver = extra.drop(val.index)\n",
    "\n",
    "print(f\"{len(pairs)} total pairs\")\n",
    "print(f\"{len(train)} training pairs\")\n",
    "print(f\"{len(val)} validation pairs\")\n",
    "print(f\"{len(ver)} verification pairs\")"
   ],
   "metadata": {
    "id": "Gpf4aKQtuwyz",
    "outputId": "021b3ef5-ee97-40d9-fe44-0a555e423562",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:45:44.031228Z",
     "iopub.execute_input": "2023-06-19T09:45:44.031606Z",
     "iopub.status.idle": "2023-06-19T09:45:49.038870Z",
     "shell.execute_reply.started": "2023-06-19T09:45:44.031578Z",
     "shell.execute_reply": "2023-06-19T09:45:49.037917Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "3092956 total pairs\n2474365 training pairs\n463943 validation pairs\n154648 verification pairs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorise text and pickle output for later use"
   ],
   "metadata": {
    "id": "FZRDCQ3Cuwyz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "size = 15000\n",
    "seq_len = 50\n",
    "batch = 64\n",
    "\n",
    "en_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len)\n",
    "ga_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len+1)\n",
    "\n",
    "en_vec.adapt(pairs[\"English\"])\n",
    "ga_vec.adapt(pairs[\"Gaeilge\"])\n",
    "\n",
    "pickle.dump({'config': en_vec.get_config(),\n",
    "             'weights': en_vec.get_weights()}\n",
    "            , open(\"en_vec_filtered.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump({'config': ga_vec.get_config(),\n",
    "             'weights': ga_vec.get_weights()}\n",
    "            , open(\"ga_vec_filtered.pkl\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load pickled configs and weights onto new TextVectorization layers"
   ],
   "metadata": {
    "id": "8rnKh2IAuwy0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "size = 15000\n",
    "seq_len = 50\n",
    "batch = 64\n",
    "\n",
    "en_pkl = pickle.load(open(\"/kaggle/input/para-weights/en_vec.pkl\", \"rb\"))\n",
    "ga_pkl = pickle.load(open(\"/kaggle/input/para-weights/ga_vec.pkl\", \"rb\"))\n",
    "\n",
    "en_vec = layers.TextVectorization.from_config(en_pkl['config'])\n",
    "ga_vec = layers.TextVectorization.from_config(ga_pkl['config'])\n",
    "\n",
    "en_vec.set_weights(en_pkl['weights'])\n",
    "ga_vec.set_weights(ga_pkl['weights'])"
   ],
   "metadata": {
    "id": "MI8pmuaMuwy0",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:03.838005Z",
     "iopub.execute_input": "2023-06-19T09:46:03.838367Z",
     "iopub.status.idle": "2023-06-19T09:46:06.423189Z",
     "shell.execute_reply.started": "2023-06-19T09:46:03.838338Z",
     "shell.execute_reply": "2023-06-19T09:46:06.422249Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def format_dataset(en, ga):\n",
    "    en = en_vec(en)\n",
    "    ga = ga_vec(ga)\n",
    "    return ({\"encoder_inputs\": en, \"decoder_inputs\": ga[:, :-1],}, ga[:, 1:])\n",
    "\n",
    "def make_dataset(en, ga):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((en, ga))\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "ds = make_dataset(train[\"English\"], train[\"Gaeilge\"])\n",
    "val_ds = make_dataset(val[\"English\"], val[\"Gaeilge\"])"
   ],
   "metadata": {
    "id": "rEh0Y9Kiuwy1",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:15.597156Z",
     "iopub.execute_input": "2023-06-19T09:46:15.598026Z",
     "iopub.status.idle": "2023-06-19T09:46:19.205126Z",
     "shell.execute_reply.started": "2023-06-19T09:46:15.597986Z",
     "shell.execute_reply": "2023-06-19T09:46:19.204112Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This sequence-to-sequence (Seq2Seq) Transformer comprises of a TransformerEncoder and a TransformerDecoder that are connected in a chain. The model also includes a PositionalEmbedding layer to incorporate information about the order of words.\n",
    "\n",
    "The TransformerEncoder takes the original sequence as input and generates a new representation of the sequence. The TransformerDecoder then takes this modified representation and the current target sequence, which includes words from 0 to N. The objective of the TransformerDecoder is to predict the next words in the target sequence, up to N+1.\n",
    "\n",
    "To accomplish this, the TransformerDecoder applies causal masking. This is crucial because it ensures that the model only uses data from target tokens 0 to N while predicting token N+1. Causal masking is necessary because the TransformerDecoder sees the entire sequence at once and must be prevented from using information from future tokens when making predictions."
   ],
   "metadata": {
    "id": "Q5rZNPW6uwy1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ],
   "metadata": {
    "id": "IlW0qGzOuwy1",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:19.207153Z",
     "iopub.execute_input": "2023-06-19T09:46:19.207508Z",
     "iopub.status.idle": "2023-06-19T09:46:19.231286Z",
     "shell.execute_reply.started": "2023-06-19T09:46:19.207476Z",
     "shell.execute_reply": "2023-06-19T09:46:19.230438Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 64\n",
    "\n",
    "def create_model(embed_dim, latent_dim, num_heads):\n",
    "    encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    pos = PositionalEmbedding(seq_len, size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(pos)\n",
    "    encoder = Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(seq_len, size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(size, activation=\"softmax\")(x)\n",
    "    decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "    \n",
    "    return transformer"
   ],
   "metadata": {
    "id": "idJyAeB0uwy2",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:19.232698Z",
     "iopub.execute_input": "2023-06-19T09:46:19.233207Z",
     "iopub.status.idle": "2023-06-19T09:46:19.248955Z",
     "shell.execute_reply.started": "2023-06-19T09:46:19.233174Z",
     "shell.execute_reply": "2023-06-19T09:46:19.247770Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next block trains model!"
   ],
   "metadata": {
    "id": "Xn9dGM4Uuwy2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TRAINING MODEL\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "transformer = create_model(embed_dim, latent_dim, num_heads)\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    optimizer=Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "cb = ModelCheckpoint(\"check\",\n",
    "                    save_weights_only=True, \n",
    "                    save_best_only=True, \n",
    "                    verbose=1, \n",
    "                    monitor=\"accuracy\", \n",
    "                    save_freq=\"epoch\", \n",
    "                    mode=\"max\" )\n",
    "\n",
    "transformer.fit(ds, epochs=epochs, validation_data=val_ds, callbacks = [cb])\n",
    "\n",
    "transformer.save_weights(\"PARAweights\")"
   ],
   "metadata": {
    "id": "89mnAK_Nuwy3",
    "outputId": "44168793-113e-4db6-97b5-74ad835f359e",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ga_vocab = ga_vec.get_vocabulary()\n",
    "ga_index_lookup = dict(zip(range(len(ga_vocab)), ga_vocab))\n",
    "max_decoded_sentence_length = seq_len\n",
    "\n",
    "\n",
    "def decode_sequence(model, input_sentence):\n",
    "    tokenized_input_sentence = en_vec([preproc_en(input_sentence)])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ga_vec([decoded_sentence])[:, :-1]\n",
    "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ga_index_lookup[sampled_token_index]\n",
    "        \n",
    "        if sampled_token == \"end\":\n",
    "            break\n",
    "            \n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "    return decoded_sentence[8:]\n"
   ],
   "metadata": {
    "id": "8q9Kqo17uwy3",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:21.752064Z",
     "iopub.execute_input": "2023-06-19T09:46:21.752604Z",
     "iopub.status.idle": "2023-06-19T09:46:21.830072Z",
     "shell.execute_reply.started": "2023-06-19T09:46:21.752565Z",
     "shell.execute_reply": "2023-06-19T09:46:21.829052Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run this to load pre-saved weights to model"
   ],
   "metadata": {
    "id": "Iw78fJXwuwy3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = create_model(embed_dim, latent_dim, num_heads)\n",
    "new_model.load_weights(\"/kaggle/input/para-weights/para-base-check\")\n",
    "new_model"
   ],
   "metadata": {
    "id": "pU3Xon-tuwy4",
    "outputId": "79d2ef61-9eb0-46a7-8d21-83047b4379b0",
    "execution": {
     "iopub.status.busy": "2023-06-19T09:46:35.324682Z",
     "iopub.execute_input": "2023-06-19T09:46:35.325407Z",
     "iopub.status.idle": "2023-06-19T09:46:38.782619Z",
     "shell.execute_reply.started": "2023-06-19T09:46:35.325374Z",
     "shell.execute_reply": "2023-06-19T09:46:38.781700Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.engine.functional.Functional at 0x7a64f2c16710>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    try:\n",
    "        inp = \" \".join(nltk.word_tokenize(random.choice(pairs[\"English\"])))\n",
    "        translated = decode_sequence(new_model, inp)\n",
    "        print(inp, \"\\n\", \"--> \", translated[8:], \"\\n\")\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "id": "HMMhlnc6uwy4",
    "execution": {
     "iopub.status.busy": "2023-05-28T09:00:54.653874Z",
     "iopub.execute_input": "2023-05-28T09:00:54.654246Z",
     "iopub.status.idle": "2023-05-28T09:01:00.974092Z",
     "shell.execute_reply.started": "2023-05-28T09:00:54.654218Z",
     "shell.execute_reply": "2023-05-28T09:01:00.972883Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "county council minutes 8 october 2001 \n -->  riscí na comhairle contae fómhair 2001 \n\nmeet the 10 most affluent countries in the world rich happy and healthy \n -->   ar na tíortha is mó san domhan is mó agus an domhain \n\namendment of section 5 of the shannon fisheries act 1938 \n -->   alt 5 den acht um [UNK] 1936 \n\nevolution of the price of the currency czech koruna compared with the currency price latvian lats \n -->  an praghas ar an airgeadra dong i gcomparáid leis an bpraghas airgeadra lats laitvis \n\nhouse of commons parliamentary papers \n -->   [UNK] [UNK] \n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "evaluation = pd.read_csv(\"/kaggle/input/nmt-data/eval.csv\", usecols=[1,2])\n",
    "\n",
    "eval_en = list(evaluation[\"English\"])\n",
    "\n",
    "references = [[preproc_ga(ga)[8:].split(\".\")[0][:-1]] for ga in list(evaluation[\"Gaeilge\"])]\n",
    "\n",
    "translations = [decode_sequence(new_model, en) for en in eval_en]\n",
    "\n",
    "translations[:5]"
   ],
   "metadata": {
    "id": "6MMYwqE3uwy4",
    "outputId": "d531a846-b3c1-4b4a-c7bb-97bc7346f4fe",
    "execution": {
     "iopub.status.busy": "2023-06-04T10:47:53.606832Z",
     "iopub.execute_input": "2023-06-04T10:47:53.607383Z",
     "iopub.status.idle": "2023-06-04T11:11:42.286773Z",
     "shell.execute_reply.started": "2023-06-04T10:47:53.607315Z",
     "shell.execute_reply": "2023-06-04T11:11:42.285296Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ag an [UNK] go dtí an [UNK]',\n 'níl mé ag mé i mo mhúinteoir',\n 'tá muid',\n 'tá mé ag foghlaim',\n 'tá mé teaghlaigh']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "bleu = corpus_bleu(references, translations, smoothing_function=SmoothingFunction().method7)\n",
    "\n",
    "bleu"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T11:48:25.468796Z",
     "iopub.execute_input": "2023-06-04T11:48:25.469289Z",
     "iopub.status.idle": "2023-06-04T11:48:25.734420Z",
     "shell.execute_reply.started": "2023-06-04T11:48:25.469250Z",
     "shell.execute_reply": "2023-06-04T11:48:25.733176Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.4140798100529444"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "open('r-para-cleaned.txt', 'w').write(\"\\n\".join([ref[0] for ref in references]))\n",
    "\n",
    "open('t-para-cleaned.txt', 'w').write(\"\\n\".join(translations))"
   ],
   "metadata": {
    "id": "mwJxad8euwy4",
    "execution": {
     "iopub.status.busy": "2023-06-04T11:11:42.584628Z",
     "iopub.execute_input": "2023-06-04T11:11:42.584998Z",
     "iopub.status.idle": "2023-06-04T11:11:42.596311Z",
     "shell.execute_reply.started": "2023-06-04T11:11:42.584968Z",
     "shell.execute_reply": "2023-06-04T11:11:42.594934Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "30142"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "decode_sequence(new_model, \"I went to the shop\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-19T09:47:05.830975Z",
     "iopub.execute_input": "2023-06-19T09:47:05.831345Z",
     "iopub.status.idle": "2023-06-19T09:47:09.552900Z",
     "shell.execute_reply.started": "2023-06-19T09:47:05.831317Z",
     "shell.execute_reply": "2023-06-19T09:47:09.551894Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'chuaigh mé go dtí an siopa'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
