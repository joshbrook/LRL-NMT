{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T14:11:54.985218Z",
     "iopub.execute_input": "2023-05-14T14:11:54.986358Z",
     "iopub.status.idle": "2023-05-14T14:12:07.920168Z",
     "shell.execute_reply.started": "2023-05-14T14:11:54.986308Z",
     "shell.execute_reply": "2023-05-14T14:12:07.918976Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cgpt = pd.read_csv(\"data/CGPT-corpus.csv\", usecols=[1,2], dtype=str)\n",
    "dcep = pd.read_csv(\"data/DCEP.txt\", sep='\\t', names=['English', 'Gaeilge'], dtype=str)\n",
    "dgt = pd.read_csv(\"data/DGT.csv\", names=['English', 'Gaeilge'], header=0, dtype=str)\n",
    "tatoeba = pd.read_csv(\"data/tatoeba.tsv\", sep='\\t', usecols=[1,3], names=['English', 'Gaeilge'], dtype=str)\n",
    "aug = pd.read_csv(\"data/augmented.csv\", usecols=[1,2], dtype=str)\n",
    "\n",
    "pairs = pd.concat([cgpt, tatoeba, dcep, dgt, aug], ignore_index=True).sample(frac=1)\n",
    "pairs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T14:13:26.532041Z",
     "iopub.execute_input": "2023-05-14T14:13:26.532842Z",
     "iopub.status.idle": "2023-05-14T14:13:27.245808Z",
     "shell.execute_reply.started": "2023-05-14T14:13:26.532801Z",
     "shell.execute_reply": "2023-05-14T14:13:27.244832Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                  English  \\\n40810   Such a proposal shall require for adoption the...   \n15986   Where the committee decides to submit the prop...   \n63356   For programme-specific result indicators, whic...   \n29884   Characteristics/remarks: Companies or groups o...   \n41734   The intention to move inadmissibility shall be...   \n...                                                   ...   \n124908                             RESULT OF VERIFICATION   \n20414   Nomination of Judges and Advocates-General at ...   \n51036   technical services which supervise the tests r...   \n123342  none of the percentages given in the list for ...   \n124987                                      Natural honey   \n\n                                                  Gaeilge  \n40810   Chun go nglacfar le togra den sórt sin, beidh ...  \n15986   I gcás ina gcinnfidh an coiste an togra a chur...  \n63356   Maidir le táscairí toraidh clár-shonracha, a b...  \n29884   Saintréithe/barúlacha: Cuideachtaí nó grúpaí c...  \n41734   Tabharfar fógra don Uachtarán 24 huaire an chl...  \n...                                                   ...  \n124908                               TORADH AN FHÍORAITHE  \n20414   Ainmniú na mBreithiúna agus na nAbhcóidí Ginea...  \n51036   seirbhísí teicniúla a dhéanann maoirseacht ar ...  \n123342  nach sárófar aon cheann de na céatadán atá lea...  \n124987                                       Mil aiceanta  \n\n[139625 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Gaeilge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40810</th>\n      <td>Such a proposal shall require for adoption the...</td>\n      <td>Chun go nglacfar le togra den sórt sin, beidh ...</td>\n    </tr>\n    <tr>\n      <th>15986</th>\n      <td>Where the committee decides to submit the prop...</td>\n      <td>I gcás ina gcinnfidh an coiste an togra a chur...</td>\n    </tr>\n    <tr>\n      <th>63356</th>\n      <td>For programme-specific result indicators, whic...</td>\n      <td>Maidir le táscairí toraidh clár-shonracha, a b...</td>\n    </tr>\n    <tr>\n      <th>29884</th>\n      <td>Characteristics/remarks: Companies or groups o...</td>\n      <td>Saintréithe/barúlacha: Cuideachtaí nó grúpaí c...</td>\n    </tr>\n    <tr>\n      <th>41734</th>\n      <td>The intention to move inadmissibility shall be...</td>\n      <td>Tabharfar fógra don Uachtarán 24 huaire an chl...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124908</th>\n      <td>RESULT OF VERIFICATION</td>\n      <td>TORADH AN FHÍORAITHE</td>\n    </tr>\n    <tr>\n      <th>20414</th>\n      <td>Nomination of Judges and Advocates-General at ...</td>\n      <td>Ainmniú na mBreithiúna agus na nAbhcóidí Ginea...</td>\n    </tr>\n    <tr>\n      <th>51036</th>\n      <td>technical services which supervise the tests r...</td>\n      <td>seirbhísí teicniúla a dhéanann maoirseacht ar ...</td>\n    </tr>\n    <tr>\n      <th>123342</th>\n      <td>none of the percentages given in the list for ...</td>\n      <td>nach sárófar aon cheann de na céatadán atá lea...</td>\n    </tr>\n    <tr>\n      <th>124987</th>\n      <td>Natural honey</td>\n      <td>Mil aiceanta</td>\n    </tr>\n  </tbody>\n</table>\n<p>139625 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preproc_en(text):\n",
    "    x = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-záéíóú ])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(text))\n",
    "    return \" \".join(nltk.word_tokenize(contractions.fix(x.lower())))\n",
    "\n",
    "def preproc_ga(text):\n",
    "    x = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-záéíóú ])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", str(text))\n",
    "    return \"[start] \" + \" \".join(nltk.word_tokenize(x.lower())) + \" [end]\"\n",
    "\n",
    "#pairs[\"English\"] = pairs[\"English\"].apply(preproc_en)\n",
    "#pairs[\"Gaeilge\"] = pairs[\"Gaeilge\"].apply(preproc_ga)\n",
    "\n",
    "pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train = pairs.sample(frac=0.8)\n",
    "val = pairs.drop(train.index)\n",
    "\n",
    "print(f\"{len(pairs)} total pairs\")\n",
    "print(f\"{len(train)} training pairs\")\n",
    "print(f\"{len(val)} validation pairs\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T14:13:33.114282Z",
     "iopub.execute_input": "2023-05-14T14:13:33.114754Z",
     "iopub.status.idle": "2023-05-14T14:13:33.206319Z",
     "shell.execute_reply.started": "2023-05-14T14:13:33.114713Z",
     "shell.execute_reply": "2023-05-14T14:13:33.205002Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "139625 total pairs\n111700 training pairs\n27925 validation pairs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorise text and pickle output for later use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "size = 15000\n",
    "seq_len = 30\n",
    "batch = 64\n",
    "\n",
    "en_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len)\n",
    "ga_vec = layers.TextVectorization(max_tokens=size, output_mode=\"int\", output_sequence_length=seq_len+1)\n",
    "\n",
    "en_vec.adapt(pairs[\"English\"])\n",
    "ga_vec.adapt(pairs[\"Gaeilge\"])\n",
    "\n",
    "pickle.dump({'config': en_vec.get_config(),\n",
    "             'weights': en_vec.get_weights()}\n",
    "            , open(\"en_vec.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump({'config': ga_vec.get_config(),\n",
    "             'weights': ga_vec.get_weights()}\n",
    "            , open(\"ga_vec.pkl\", \"wb\"))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:08:51.110692Z",
     "iopub.execute_input": "2023-05-14T10:08:51.111390Z",
     "iopub.status.idle": "2023-05-14T10:09:06.686376Z",
     "shell.execute_reply.started": "2023-05-14T10:08:51.111350Z",
     "shell.execute_reply": "2023-05-14T10:09:06.685287Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load pickled configs and weights onto new TextVectorization layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "en_pkl = pickle.load(open(\"en_vec.pkl\", \"rb\"))\n",
    "ga_pkl = pickle.load(open(\"ga_vec.pkl\", \"rb\"))\n",
    "\n",
    "en_vec = layers.TextVectorization.from_config(en_pkl['config'])\n",
    "ga_vec = layers.TextVectorization.from_config(ga_pkl['config'])\n",
    "\n",
    "en_vec.set_weights(en_pkl['weights'])\n",
    "ga_vec.set_weights(ga_pkl['weights'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:09:06.688486Z",
     "iopub.execute_input": "2023-05-14T10:09:06.688810Z",
     "iopub.status.idle": "2023-05-14T10:09:06.718760Z",
     "shell.execute_reply.started": "2023-05-14T10:09:06.688781Z",
     "shell.execute_reply": "2023-05-14T10:09:06.717816Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def format_dataset(en, ga):\n",
    "    en = en_vec(en)\n",
    "    ga = ga_vec(ga)\n",
    "    return ({\"encoder_inputs\": en, \"decoder_inputs\": ga[:, :-1],}, ga[:, 1:])\n",
    "\n",
    "def make_dataset(en, ga):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((en, ga))\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "ds = make_dataset(train[\"English\"], train[\"Gaeilge\"])\n",
    "val_ds = make_dataset(val[\"English\"], val[\"Gaeilge\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:09:06.722268Z",
     "iopub.execute_input": "2023-05-14T10:09:06.722552Z",
     "iopub.status.idle": "2023-05-14T10:09:06.981574Z",
     "shell.execute_reply.started": "2023-05-14T10:09:06.722525Z",
     "shell.execute_reply": "2023-05-14T10:09:06.980453Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This sequence-to-sequence (Seq2Seq) Transformer comprises of a TransformerEncoder and a TransformerDecoder that are connected in a chain. The model also includes a PositionalEmbedding layer to incorporate information about the order of words.\n",
    "\n",
    "The TransformerEncoder takes the original sequence as input and generates a new representation of the sequence. The TransformerDecoder then takes this modified representation and the current target sequence, which includes words from 0 to N. The objective of the TransformerDecoder is to predict the next words in the target sequence, up to N+1.\n",
    "\n",
    "To accomplish this, the TransformerDecoder applies causal masking. This is crucial because it ensures that the model only uses data from target tokens 0 to N while predicting token N+1. Causal masking is necessary because the TransformerDecoder sees the entire sequence at once and must be prevented from using information from future tokens when making predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:09:06.984116Z",
     "iopub.execute_input": "2023-05-14T10:09:06.984688Z",
     "iopub.status.idle": "2023-05-14T10:09:07.012021Z",
     "shell.execute_reply.started": "2023-05-14T10:09:06.984646Z",
     "shell.execute_reply": "2023-05-14T10:09:07.010752Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 64\n",
    "\n",
    "def create_model(embed_dim, latent_dim, num_heads):\n",
    "    encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    pos = PositionalEmbedding(seq_len, size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(pos)\n",
    "    encoder = Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "    decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(seq_len, size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(size, activation=\"softmax\")(x)\n",
    "    decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "    \n",
    "    return transformer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:09:07.015358Z",
     "iopub.execute_input": "2023-05-14T10:09:07.015744Z",
     "iopub.status.idle": "2023-05-14T10:09:07.026965Z",
     "shell.execute_reply.started": "2023-05-14T10:09:07.015708Z",
     "shell.execute_reply": "2023-05-14T10:09:07.025910Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# TRAINING MODEL\n",
    "\n",
    "epochs = 30  # This should be at least 30 for convergence\n",
    "\n",
    "transformer = create_model(embed_dim, latent_dim, num_heads)\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    optimizer=Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(ds, epochs=epochs, validation_data=val_ds)\n",
    "transformer.save_weights(\"weights\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T10:09:43.981451Z",
     "iopub.execute_input": "2023-05-14T10:09:43.982197Z",
     "iopub.status.idle": "2023-05-14T11:30:22.507700Z",
     "shell.execute_reply.started": "2023-05-14T10:09:43.982156Z",
     "shell.execute_reply": "2023-05-14T11:30:22.506622Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"transformer\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n                                                                                                  \n positional_embedding (Position  (None, None, 256)   3847680     ['encoder_inputs[0][0]']         \n alEmbedding)                                                                                     \n                                                                                                  \n decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n                                                                                                  \n transformer_encoder (Transform  (None, None, 256)   17878528    ['positional_embedding[0][0]']   \n erEncoder)                                                                                       \n                                                                                                  \n model_1 (Functional)           (None, None, 15000)  42408344    ['decoder_inputs[0][0]',         \n                                                                  'transformer_encoder[0][0]']    \n                                                                                                  \n==================================================================================================\nTotal params: 64,134,552\nTrainable params: 64,134,552\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/30\n625/625 [==============================] - 155s 238ms/step - loss: 6.2234 - accuracy: 0.0696 - val_loss: 5.9733 - val_accuracy: 0.0738\nEpoch 2/30\n625/625 [==============================] - 148s 237ms/step - loss: 5.9403 - accuracy: 0.0766 - val_loss: 5.7662 - val_accuracy: 0.0854\nEpoch 3/30\n625/625 [==============================] - 148s 237ms/step - loss: 5.2210 - accuracy: 0.1263 - val_loss: 4.6923 - val_accuracy: 0.1701\nEpoch 4/30\n625/625 [==============================] - 148s 237ms/step - loss: 4.5486 - accuracy: 0.1826 - val_loss: 4.2717 - val_accuracy: 0.2042\nEpoch 5/30\n625/625 [==============================] - 148s 237ms/step - loss: 4.2360 - accuracy: 0.2090 - val_loss: 4.0526 - val_accuracy: 0.2237\nEpoch 6/30\n625/625 [==============================] - 148s 237ms/step - loss: 4.0292 - accuracy: 0.2281 - val_loss: 3.8447 - val_accuracy: 0.2479\nEpoch 7/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.8671 - accuracy: 0.2449 - val_loss: 3.7166 - val_accuracy: 0.2612\nEpoch 8/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.7281 - accuracy: 0.2590 - val_loss: 3.5622 - val_accuracy: 0.2801\nEpoch 9/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.5933 - accuracy: 0.2739 - val_loss: 3.4399 - val_accuracy: 0.2967\nEpoch 10/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.4586 - accuracy: 0.2909 - val_loss: 3.3388 - val_accuracy: 0.3096\nEpoch 11/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.3317 - accuracy: 0.3066 - val_loss: 3.1873 - val_accuracy: 0.3327\nEpoch 12/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.1953 - accuracy: 0.3252 - val_loss: 3.0526 - val_accuracy: 0.3534\nEpoch 13/30\n625/625 [==============================] - 148s 237ms/step - loss: 3.0783 - accuracy: 0.3411 - val_loss: 2.9503 - val_accuracy: 0.3661\nEpoch 14/30\n625/625 [==============================] - 148s 237ms/step - loss: 2.9817 - accuracy: 0.3546 - val_loss: 2.8521 - val_accuracy: 0.3845\nEpoch 15/30\n625/625 [==============================] - 148s 237ms/step - loss: 2.8721 - accuracy: 0.3711 - val_loss: 2.7514 - val_accuracy: 0.4002\nEpoch 16/30\n625/625 [==============================] - 148s 237ms/step - loss: 2.7850 - accuracy: 0.3845 - val_loss: 2.6585 - val_accuracy: 0.4173\nEpoch 17/30\n625/625 [==============================] - 148s 237ms/step - loss: 2.6859 - accuracy: 0.4000 - val_loss: 2.5473 - val_accuracy: 0.4388\nEpoch 18/30\n625/625 [==============================] - 148s 238ms/step - loss: 2.5997 - accuracy: 0.4142 - val_loss: 2.4597 - val_accuracy: 0.4552\nEpoch 19/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.5166 - accuracy: 0.4291 - val_loss: 2.3932 - val_accuracy: 0.4680\nEpoch 20/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.4599 - accuracy: 0.4380 - val_loss: 2.3283 - val_accuracy: 0.4810\nEpoch 21/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.3873 - accuracy: 0.4507 - val_loss: 2.2722 - val_accuracy: 0.4949\nEpoch 22/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.3269 - accuracy: 0.4614 - val_loss: 2.2055 - val_accuracy: 0.5065\nEpoch 23/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.2780 - accuracy: 0.4705 - val_loss: 2.1893 - val_accuracy: 0.5071\nEpoch 24/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.2329 - accuracy: 0.4777 - val_loss: 2.1061 - val_accuracy: 0.5269\nEpoch 25/30\n625/625 [==============================] - 155s 247ms/step - loss: 2.1654 - accuracy: 0.4905 - val_loss: 2.0513 - val_accuracy: 0.5390\nEpoch 26/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.1385 - accuracy: 0.4952 - val_loss: 1.9989 - val_accuracy: 0.5512\nEpoch 27/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.0856 - accuracy: 0.5063 - val_loss: 1.9879 - val_accuracy: 0.5524\nEpoch 28/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.0373 - accuracy: 0.5152 - val_loss: 1.9203 - val_accuracy: 0.5676\nEpoch 29/30\n625/625 [==============================] - 149s 238ms/step - loss: 2.0010 - accuracy: 0.5228 - val_loss: 1.8991 - val_accuracy: 0.5728\nEpoch 30/30\n625/625 [==============================] - 149s 238ms/step - loss: 1.9632 - accuracy: 0.5309 - val_loss: 1.8466 - val_accuracy: 0.5868\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ga_vocab = ga_vec.get_vocabulary()\n",
    "ga_index_lookup = dict(zip(range(len(ga_vocab)), ga_vocab))\n",
    "max_decoded_sentence_length = seq_len\n",
    "\n",
    "\n",
    "def decode_sequence(model, input_sentence):\n",
    "    tokenized_input_sentence = en_vec([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ga_vec([decoded_sentence])[:, :-1]\n",
    "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ga_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T11:30:22.510345Z",
     "iopub.execute_input": "2023-05-14T11:30:22.510741Z",
     "iopub.status.idle": "2023-05-14T11:30:22.554936Z",
     "shell.execute_reply.started": "2023-05-14T11:30:22.510701Z",
     "shell.execute_reply": "2023-05-14T11:30:22.553991Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load pre-saved weights to model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = create_model(embed_dim, latent_dim, num_heads)\n",
    "new_model.load_weights(\"/kaggle/input/seq2seqweights/weights\")\n",
    "new_model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-13T11:31:48.858789Z",
     "iopub.execute_input": "2023-05-13T11:31:48.859607Z",
     "iopub.status.idle": "2023-05-13T11:31:52.826604Z",
     "shell.execute_reply.started": "2023-05-13T11:31:48.859562Z",
     "shell.execute_reply": "2023-05-13T11:31:52.825554Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.engine.functional.Functional at 0x7d4a8c332d50>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    try:\n",
    "        inp = \" \".join(nltk.word_tokenize(random.choice(pairs[\"English\"])))\n",
    "        translated = decode_sequence(transformer, inp)\n",
    "        print(inp, \"\\n\", \"--> \", translated[8:], \"\\n\")\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T11:30:22.556378Z",
     "iopub.execute_input": "2023-05-14T11:30:22.556961Z",
     "iopub.status.idle": "2023-05-14T11:30:30.855250Z",
     "shell.execute_reply.started": "2023-05-14T11:30:22.556919Z",
     "shell.execute_reply": "2023-05-14T11:30:30.854101Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "from the Chamber \n -->  na gceisteanna                             \n\nThe President shall forward the preliminary draft estimates to the committee responsible , which shall draw up the draft estimates and report to Parliament . \n -->  an tuachtarán an rún as na meastacháin ar aghaidh chuig an gcoiste freagrach a tharraingt suas ar an dréacht den chlár oibre       i  \n\nBetween two and eight Members chosen by lot shall count the votes cast in a secret ballot , unless an electronic vote is taken . \n -->  le bhunú idir na feisirí a tharraingt siar go sealadach ar a chaitear agus beidh an dara ballóid rúnda a chomhaireamh  sa  i don don don sa  \n\nEach Member of the Commission shall make sure that there is a regular and direct flow of information between the Member of the Commission and the chair of the relevant parliamentary committee . \n -->  le linn an choimisiúin agus na caestóirí a bheidh a bheidh a bheidh a bheidh a chur ar an choimisiúin a chur ar aghaidh chuig an gcoimisiún  i a \n\nIt shall enter into force on the date of its publication . \n -->  sé in iúl i bhfeidhm a fhoilsiú in      in sa   sa  sa        sa sa  \n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "sample = val.sample(30)\n",
    "bleu = []\n",
    "for en, ga in zip(sample[\"English\"], sample[\"Gaeilge\"]):\n",
    "    if len(en) < 60:\n",
    "        try:\n",
    "            en_tk = \" \".join(nltk.word_tokenize(en))\n",
    "            ga_tk = nltk.word_tokenize(ga.lower())\n",
    "\n",
    "            smoothie = SmoothingFunction().method7\n",
    "            translation = decode_sequence(transformer, en_tk)[8:]\n",
    "            score = sentence_bleu([ga_tk], translation.split(), smoothing_function=smoothie)\n",
    "\n",
    "            print(en_tk, \"\\n\", \" \".join(ga_tk), \"\\n\", translation, \"\\n\", score, \"\\n\")\n",
    "            bleu.append(score)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "np.mean(bleu)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-14T12:22:24.267027Z",
     "iopub.execute_input": "2023-05-14T12:22:24.267424Z",
     "iopub.status.idle": "2023-05-14T12:22:40.244164Z",
     "shell.execute_reply.started": "2023-05-14T12:22:24.267389Z",
     "shell.execute_reply": "2023-05-14T12:22:40.243054Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Committee on Culture and Education \n an coiste um chultúr agus um oideachas \n coiste um chultúr agus um oideachas  sa          sa    um um        \n 0.6089767658837411 \n\nRequirements for the drafting of legislative acts \n ceanglais maidir le dréachtú gníomhartha reachtacha \n maidir le dréachtú gníomhartha reachtacha  reachtacha   reachtacha           reachtacha reachtacha reachtacha      i  \n 0.4326725975388602 \n\nRule 68 ( 2 ) , ( 4 ) , ( 5 ) , ( 7 ) and ( 8 ) shall apply . \n beidh feidhm ag riail 68 ( 2 ) , ( 4 ) , ( 5 ) , ( 7 ) agus ( 8 ) . \n feidhm ag parlaimint na nÓsanna imeachta seo i gcomhréir le rialacha nós imeachta seo    i          don i  \n 0.15732700150487738 \n\nProvisional Chair \n an cathaoirleach sealadach \n cathaoirleach sealadach                             \n 0.29224668520773955 \n\nFormation of political groups \n bunú na ngrúpaí polaitiúla \n na ngrúpaí polaitiúla                            \n 0.5410492696833055 \n\nDuties of the Bureau \n dualgais an bhiúró \n an bhiúró                             \n 0.29224668520773955 \n\nYes , I live in Istanbul . \n tá , tá mé i mo chónaí in iostanbúl . \n mé ag scríobh                         ag   \n 0.059141270773256954 \n\nDisruption outside the Chamber \n disruption outside the chamber \n a atáirgeadh agus don seomra                          \n 0 \n\nDuties of the Bureau \n dualgais an bhiúró \n an bhiúró                             \n 0.29224668520773955 \n\n",
     "output_type": "stream"
    },
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.29732299566747333"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
